---
type: Concept
---

Hallucinations in large language models refer to the generation of false or misleading information that is not grounded in the provided source material. This phenomenon can erode trust in LLMs and limit their real-world applications. Benchmarks like FACTS Grounding aim to evaluate and reduce such hallucinations.