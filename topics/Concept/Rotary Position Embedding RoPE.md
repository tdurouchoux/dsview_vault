---
type: Concept
---

Rotary Position Embedding (RoPE) is a technique used in transformer-based language models to encode positional information. Unlike traditional absolute positional embeddings, RoPE encodes position by rotating the query and key vectors in a way that depends on each token's position. This method was introduced as an alternative to absolute positional embeddings and has become widely adopted in modern large language models (LLMs) since its introduction in 2021 and popularization with the Llama model in 2023. RoPE is considered elegant but can be tricky to explain and is designed to improve the model's understanding of token positions without the need for separate positional embedding vectors. Rotary Positional Embeddings (RoPE) is a technique used in ModernBERT and transformer models to improve the model's understanding of the relative positions of words in a sequence. Unlike traditional positional encodings, RoPE allows the model to better capture the contextual relationships between tokens, enabling it to handle longer sequence lengths more effectively. This method is inspired by advancements in transformer architectures and contributes to the model's ability to process and understand long-context inputs efficiently. It applies rotary matrices to the input embeddings, allowing the model to better capture positional information and improve throughput, particularly useful for tasks involving long sequences of data.