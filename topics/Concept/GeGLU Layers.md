---
type: Concept
---

GeGLU Layers are an improved version of the original BERT's GeLU (Gaussian Error Linear Units) activation function used in ModernBERT. These layers enhance the model's performance by providing a more efficient and effective activation mechanism. By replacing the older MLP layers with GeGLU layers, ModernBERT achieves better computational efficiency and accuracy, contributing to its overall performance improvements over previous models like BERT and DeBERTaV3.