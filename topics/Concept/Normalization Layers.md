---
type: Concept
---

Normalization layers are techniques used in neural networks to standardize the inputs to a layer for each mini-batch. This process helps to stabilize and often significantly accelerate the training of neural networks. Common types of normalization layers include Batch Normalization, Layer Normalization, Instance Normalization, and Group Normalization. Batch Normalization normalizes the inputs across the batch dimension, Layer Normalization normalizes across the features, Instance Normalization normalizes per sample and channel, and Group Normalization divides the channels into groups and normalizes within each group. These techniques help in reducing internal covariate shift, which is the change in the distribution of network activations due to the change in network parameters during training. By normalizing the inputs, these layers can lead to faster and more stable training, often allowing for higher learning rates and reducing the sensitivity to initialization.