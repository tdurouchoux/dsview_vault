---
type: Concept
---

Optimizers are algorithms used in machine learning and deep learning to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers adjust the parameters of the model to minimize the loss function during training. Common optimizers include Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad. Each optimizer has its own method of updating the parameters, often involving techniques like momentum, adaptive learning rates, and gradient clipping. The choice of optimizer can significantly impact the training process, including convergence speed and the final performance of the model. Advanced optimizers like Adam, which combines the advantages of two other extensions of stochastic gradient descent, have become popular due to their efficiency and effectiveness in handling sparse gradients on noisy problems.