---
type: Concept
---

Optimizers are algorithms used to adjust the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers help in minimizing the loss function during the training of neural networks, thereby improving model performance. Common optimizers include Adam, AMSGrad, and AdaBelief.