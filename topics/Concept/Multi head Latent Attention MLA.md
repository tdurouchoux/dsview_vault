---
type: Concept
---

Multi-head Latent Attention (MLA) is an attention mechanism designed to improve the efficiency and performance of large language models. It allows the model to focus on different parts of the input sequence simultaneously, capturing a wide range of dependencies and relationships. MLA is particularly useful in models like DeepSeek-V3, where it helps achieve efficient inference and cost-effective training.