---
type: Concept
---

LLM-based evaluators are methods that use large language models (LLMs) to evaluate the quality of generated text. These evaluators leverage the capabilities of LLMs to assess various aspects of the text, such as fluency, coherence, relevance, and factual consistency. LLM-based evaluators can be used in both reference-based and reference-free evaluation scenarios. They offer several advantages, including scalability, explainability, and the ability to handle a wide range of evaluation tasks. However, they also come with challenges such as positional bias, verbosity bias, and limited mathematical and reasoning capabilities. Strategies such as Multiple Evidence Calibration (MEC), Balanced Position Calibration (BPC), and Human In The Loop Calibration (HITLC) have been proposed to mitigate these biases.