---
type: Concept
---

iRoPE (interleaved Rotary Position Embeddings) architecture is an innovation in the Llama 4 models that uses interleaved attention layers without positional embeddings to enhance length generalization. This architecture is designed to support very long context lengths, up to 10 million tokens, and includes inference-time temperature scaling of attention to improve performance on tasks requiring extensive context understanding.