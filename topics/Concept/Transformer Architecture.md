---
type: Concept
---

The Transformer architecture is a model architecture used in BERT that relies on self-attention mechanisms to process sequential data. It is designed to handle a variety of downstream tasks by swapping out the appropriate inputs and outputs. The architecture allows BERT to model many downstream tasks effectively, whether they involve single text or text pairs, by using self-attention to unify these stages.