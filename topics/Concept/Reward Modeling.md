---
type: Concept
---

Reward Modeling involves creating models that can evaluate and provide feedback on the outputs of other models. This is crucial for training models to generate high-quality responses and align with user intentions. Reward models are used to assess the quality of generated data, provide rewards or penalties, and guide the improvement of the primary model. Reward model training involves generating a reward model calibrated with human preferences. This model takes in a sequence of text and returns a scalar reward that numerically represents human preference. The training dataset for the reward model is generated by sampling prompts from a predefined dataset, passing them through the initial language model to generate text, and then having human annotators rank the generated text outputs.