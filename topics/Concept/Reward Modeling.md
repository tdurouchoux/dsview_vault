---
type: Concept
---

Reward Modeling is the process of creating reward functions that guide the behavior of language models. These reward functions are used to train models to produce desired outputs, such as following instructions accurately, generating high-quality responses, or avoiding harmful content. Reward model training involves creating a model that assigns a scalar reward to a sequence of text, representing human preferences. This is done by collecting data from human annotators who rank the outputs of language models generated from the same prompt. The reward model is then trained on this data to predict human preferences, which are used to fine-tune the language model in the reinforcement learning stage. Reward modeling can involve generating synthetic preference data to improve the model's understanding of what constitutes a good response.