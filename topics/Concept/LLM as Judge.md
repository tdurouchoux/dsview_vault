---
type: Concept
---

LLM-as-Judge is a technique used to evaluate the performance of large language models (LLMs) by using a stronger LLM to assess the outputs of weaker LLMs. This can involve presenting the stronger LLM with pairs of outputs and asking it to select the better one, or using chain-of-thought prompting to improve the reliability of the evaluation. LLM-as-Judge can be a useful tool for identifying areas for improvement and optimizing the performance of LLMs.