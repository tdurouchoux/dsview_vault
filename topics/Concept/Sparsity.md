---
type: Concept
---

Sparsity in the context of Mixture of Experts (MoE) refers to the conditional computation where only a subset of the model's parameters are activated for each input token. This approach allows for more efficient training and inference by reducing the computational load and memory requirements, as not all experts are used for every input.