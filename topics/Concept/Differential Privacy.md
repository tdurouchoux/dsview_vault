---
type: Concept
---

Differential Privacy is a privacy-preserving technique that adds controlled noise to data or model updates to prevent the identification of individual data points. In federated learning, it helps to ensure that the presence or absence of any single user's data does not significantly affect the global model, thereby protecting user privacy. This technique involves carefully bounding the impact of any possible user contribution and adding random noise to the training process.