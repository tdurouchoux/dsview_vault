---
type: Concept
---

Reinforcement Learning from Human Feedback (RLHF) is a method used to align language models with human values and preferences. It involves training a reward model based on human feedback and then using this reward model to fine-tune the language model, improving its performance on tasks that require human-like judgment.