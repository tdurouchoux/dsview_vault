---
type: Concept
---

Reinforcement Learning from Human Feedback (RLHF) is a method used to fine-tune language models based on feedback from human evaluators. It involves training the model to generate responses that align with human preferences, improving the model's performance and relevance in real-world applications. Torchtune supports RLHF for various models.