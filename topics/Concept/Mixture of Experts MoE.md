---
type: Concept
---

Mixture of Experts (MoE) architecture is a model design approach where a single token activates only a fraction of the total parameters, improving computational efficiency. This architecture is used in advanced language models like Llama 4 and DeepSeek-R1, where it allows for more efficient training and inference. In MoE models, each token is processed by a subset of experts, which reduces the overall computational cost while maintaining high performance. This approach is particularly beneficial for large models, as it allows them to achieve higher quality with a fixed training FLOPs budget compared to dense models. The MoE architecture also enhances inference efficiency by lowering model serving costs and latency. It involves a combination of dense and sparse layers, where different 'experts' specialize in different types of tasks. This approach allows the model to handle a variety of tasks more efficiently and effectively, improving overall performance and scalability. A Mixture of Experts (MoE) is a machine learning model architecture that combines multiple specialized sub-models, called experts, with a gating network that determines how to distribute a learning task among the experts. Each expert specializes in a different region of the input space, and the gating network learns to route different parts of the input to the appropriate experts. This approach allows for more efficient training and inference, as only a subset of the model's parameters are activated for any given input. MoEs are particularly useful for scaling up models while keeping computational costs manageable. MoE merging is a technique that combines multiple dense models into a mixture of experts. This approach can be used for direct deployment or further training, allowing for the integration of specialized knowledge from different models into a single, more versatile model.