---
type: Concept
---

Mixture of Experts (MoE) is a model design approach where different parts of the model, known as experts, specialize in different types of data or tasks. In MoE models, only a fraction of the total parameters are activated for a given input token, leading to improved computational efficiency and performance. This architecture is used in the Llama 4 models to enhance inference efficiency and reduce serving costs. It involves stacking Transformer decoder blocks, where the first few layers are dense and the subsequent layers are composed of mixtures of experts. The model employs multiple expert networks, each specialized in different aspects of the data, and uses a gating mechanism to selectively activate the most relevant experts for each input. This architecture allows the model to handle a wide range of tasks efficiently, leveraging specialized layers for different types of problems. This approach is particularly useful for large-scale models, as it enables the model to handle complex tasks by leveraging the strengths of different experts and allows for scaling up model or dataset size with the same compute budget as a dense model, leading to improved model quality and computational efficiency.