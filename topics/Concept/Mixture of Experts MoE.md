---
type: Concept
---

Mixture of Experts (MoE) is a model architecture used in large language models (LLMs) where multiple feed-forward modules, or 'experts,' are employed instead of a single feed-forward module. In MoE, a router selects a subset of experts for each token generation step, making the model sparse rather than dense. This approach increases the model's capacity by using a large number of parameters but keeps inference efficient by activating only a few experts at a time. MoE modules are particularly useful for scaling up models while maintaining computational efficiency, as they allow the model to leverage more parameters without significantly increasing the computational cost during inference. Mixture of Experts (MoE) is a model design approach where different parts of the model, known as experts, specialize in different types of data or tasks. In MoE models, only a fraction of the total parameters are activated for a given input token, leading to improved computational efficiency and performance. This architecture is used in the Llama 4 models to enhance inference efficiency and reduce serving costs. It involves stacking Transformer decoder blocks, where the first few layers are dense and the subsequent layers are composed of mixtures of experts. The model employs multiple expert networks, each specialized in different aspects of the data, and uses a gating mechanism to selectively activate the most relevant experts for each input. This architecture allows the model to handle a wide range of tasks efficiently, leveraging specialized layers for different types of problems. This approach is particularly useful for large-scale models, as it enables the model to handle complex tasks by leveraging the strengths of different experts and allows for scaling up model or dataset size with the same compute budget as a dense model, leading to improved model quality and computational efficiency.