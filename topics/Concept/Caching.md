---
type: Concept
---

Caching in the context of large language models (LLMs) involves storing the results of previous computations or responses to frequently asked questions to reduce latency and computational costs. By caching responses, systems can quickly retrieve and serve vetted responses, eliminating the need to regenerate them. This approach improves efficiency, reduces the risk of serving harmful or inappropriate content, and enhances the overall user experience.