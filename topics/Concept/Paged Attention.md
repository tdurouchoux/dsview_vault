---
type: Concept
---

Paged attention is a mechanism used to manage memory more efficiently during the inference of transformer models. It involves dividing the attention computation into smaller, manageable pages, which helps in reducing memory usage and improving performance. This technique is particularly useful for handling long sequences or large models. In the context of the Transformers library, paged attention is introduced to optimize inference performance.