---
type: Concept
---

Gradient accumulation is a technique used in training machine learning models, particularly in the context of large models or limited computational resources. This technique involves accumulating gradients over multiple batches of data before performing a weight update. By accumulating gradients, the model can effectively simulate a larger batch size, which can help in stabilizing and improving the training process. This is particularly useful when dealing with memory constraints or when training very large models.