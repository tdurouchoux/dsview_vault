---
type: Concept
---

FACTS Grounding is a benchmark designed to evaluate the factual accuracy and grounding capabilities of large language models (LLMs). It assesses how well LLMs can generate responses that are factually accurate and fully attributable to provided source material, without hallucinating information. The benchmark includes a dataset with various documents and user requests, and it uses multiple AI judge models to evaluate the responses.