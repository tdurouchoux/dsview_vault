---
type: Concept
---

Evaluation and monitoring are critical components of building and deploying large language models (LLMs). Evaluation involves assessing the model's performance on specific tasks or metrics, while monitoring involves tracking the model's performance over time and identifying any issues or areas for improvement. Effective evaluation and monitoring can help ensure that the model's outputs are accurate, reliable, and aligned with the user's expectations.