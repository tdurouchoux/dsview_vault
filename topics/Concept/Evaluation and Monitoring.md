---
type: Concept
---

Evaluation and Monitoring in the context of large language models (LLMs) involves assessing the quality and performance of the model's outputs and continuously tracking its behavior to ensure it meets desired standards. This includes creating assertion-based unit tests, using LLM-as-Judge for pairwise comparisons, and implementing guardrails to detect and filter inappropriate or harmful content.