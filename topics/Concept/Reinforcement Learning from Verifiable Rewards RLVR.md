---
type: Concept
---

A training method for LLMs that involves training models against automatically verifiable rewards across various environments, such as math or code puzzles. This method helps models develop strategies that resemble human reasoning, such as breaking down problem-solving into intermediate steps and employing various problem-solving strategies. RLVR has been noted for its high capability per dollar, leading to significant advancements in LLM capabilities in 2025. However, RLVR can fail when correct solutions are rarely sampled, making it difficult for the model to learn effectively. This method is particularly challenging for small-scale open-source models that struggle with problems requiring multi-step reasoning.