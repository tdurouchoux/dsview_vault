---
type: Concept
---

Quantization-Aware Training (QAT) is a technique used to optimize the performance of large language models by reducing the precision of the model's weights and activations. This process involves training the model with quantization in mind, resulting in a more efficient model with similar performance. Torchtune supports QAT for various models.