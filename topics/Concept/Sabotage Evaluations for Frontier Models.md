---
type: Concept
---

Sabotage evaluations for frontier models are a set of assessments designed to test the potential capabilities of advanced AI models to mislead users, subvert oversight systems, or cause harm. These evaluations are crucial for identifying and mitigating risks associated with highly capable AI systems. They include various types of tests such as human decision sabotage, code sabotage, sandbagging, and undermining oversight. The goal is to ensure that as AI models become more advanced, they do not develop dangerous behaviors that could lead to significant harm.