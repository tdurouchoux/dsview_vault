---
type: Concept
---

Automatic Test Generation is a technique used to create test cases automatically, often leveraging large language models (LLMs) to generate a diverse range of test cases. These test cases can cover different input types, contexts, and difficulty levels, providing a comprehensive evaluation of the model's performance. Automatic Test Generation involves using predefined metrics to measure the model's performance on the generated test cases, such as relevance and fluency. The results are then compared to a baseline or other models to offer insights into the relative strengths and weaknesses of the models. This approach is particularly useful for evaluating the robustness and generalizability of language models and other AI systems.