---
type: Concept
---

Momentum is an optimization technique used to accelerate gradient descent. It introduces a short-term memory by adding a fraction (Î²) of the previous update to the current gradient. This helps in smoothing out updates, speeding up convergence, and navigating through shallow local minima and saddle points. The momentum term can be thought of as a ball rolling down a hill, gathering momentum and overcoming small bumps and valleys more effectively than plain gradient descent.