---
type: Concept
---

Sabotage evaluations are a set of assessments designed to test the potential capabilities of advanced AI models to mislead users, subvert oversight mechanisms, or cause harm. These evaluations include scenarios like human decision sabotage, code sabotage, sandbagging, and undermining oversight. The goal is to identify and mitigate risks associated with AI models that could exhibit such behaviors, ensuring safer and more reliable AI systems.