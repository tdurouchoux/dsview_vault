---
type: Concept
---

HumanEval is a popular Python benchmark used to evaluate the performance of code-generating models. It tests the model's ability to complete functions based on their signatures and docstrings. In the context of StarCoder, HumanEval was used to measure the model's performance, and specific prompts were developed to improve its score, setting a new state-of-the-art result for open models.