---
type: Concept
---

RAG Evaluation involves assessing the performance of Retrieval-Augmented Generation (RAG) systems through various methods. Independent evaluation focuses on assessing the retrieval and generation modules separately, using metrics such as Hit Rate, MRR, NDCG, and Precision. End-to-end evaluation assesses the final response generated by the RAG model, considering factors such as answer fidelity, relevance, and harmlessness. Key metrics for RAG evaluation include Faithfulness, Answer Relevance, and Context Relevance. Evaluation frameworks such as RAGAS and ARES use large language models (LLMs) as judges to provide automated and reliable assessments of RAG systems.