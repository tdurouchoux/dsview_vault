---
type: Concept
---

Sequence Length Warmup is a technique used during the training of language models to gradually increase the length of the input sequences. This helps in stabilizing the training process and improving the model's performance on longer sequences. By slowly increasing the sequence length, the model can better adapt to the complexity of longer inputs, leading to more stable and effective training.