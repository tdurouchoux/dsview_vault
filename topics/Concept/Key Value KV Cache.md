---
type: Concept
---

A Key-Value (KV) Cache in the context of large language models (LLMs) refers to a mechanism for storing and quickly retrieving precomputed data that represents the inference state of the model. This cache is used to encapsulate the processed information from preloaded documents, allowing the model to generate responses based on this cached context without the need for real-time retrieval. The KV cache helps in reducing latency and improving the efficiency of the model's inference process.