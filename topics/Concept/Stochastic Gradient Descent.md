---
type: Concept
---

Stochastic Gradient Descent (SGD) is a variant of gradient descent that uses an estimate of the gradient computed from a randomly selected subset of the data. This makes SGD faster and more suitable for large datasets but introduces noise into the optimization process. Momentum can be applied to SGD to accelerate convergence and mitigate the noise, making it more effective in practice.