---
type: Concept
---

Stochastic Gradient Descent (SGD) is a variant of gradient descent that uses a random sample of the data to compute the gradient at each iteration. This makes SGD faster and more suitable for large datasets, as it does not require computing the gradient over the entire dataset. However, the randomness introduced by SGD can lead to noisy updates, which can be mitigated using techniques like momentum.