---
type: Concept
---

Inference-time scaling refers to the practice of increasing computational resources during the inference phase of an AI system to enhance its reasoning and output quality. This approach allows the AI system to spend more time on complex tasks, such as generating and refining hypotheses, leading to improved performance and more accurate results. It is particularly useful in scientific research where detailed and precise outputs are crucial. It can involve methods like chain-of-thought (CoT) prompting, where the model is encouraged to generate intermediate reasoning steps, or using search strategies like beam search and majority voting to refine answers. These techniques make inference more expensive but can lead to more accurate results for complex problems.