---
type: Concept
---

Deploying large language models (LLMs) involves making these models available for use in various environments, ranging from local applications to large-scale cloud infrastructures. Local deployment options include using servers like LM Studio and Ollama, which allow for private and secure model usage. For demos and prototypes, frameworks like Gradio and Streamlit are commonly used. Large-scale deployments often leverage cloud services and optimized text generation frameworks like TGI and vLLM. Edge deployment, targeting constrained environments like web browsers and mobile devices, utilizes frameworks such as MLC LLM and mnn-llm. Effective deployment strategies are essential for making LLMs accessible and useful in diverse scenarios.