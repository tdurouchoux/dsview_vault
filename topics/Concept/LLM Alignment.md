---
type: Concept
---

LLM alignment refers to the process of aligning the outputs of large language models (LLMs) with human values and intentions. This involves fine-tuning the model to ensure its responses are safe, ethical, and aligned with the user's goals. Techniques such as reinforcement learning from human feedback (RLHF) are often used in this process.