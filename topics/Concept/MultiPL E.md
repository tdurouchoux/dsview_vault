---
type: Concept
---

MultiPL-E is a benchmark that extends HumanEval to multiple programming languages. It is used to evaluate the multilingual capabilities of code-generating models like StarCoder. MultiPL-E helps assess how well a model can generate code in various programming languages, providing a comprehensive evaluation of its multilingual performance.