---
type: Concept
---

Tokenization is the process of converting text into numerical representations that LLMs can process. It involves breaking down text into smaller units, such as words or subwords, and mapping these units to numerical identifiers. This process is crucial for preparing text data for input into a language model.