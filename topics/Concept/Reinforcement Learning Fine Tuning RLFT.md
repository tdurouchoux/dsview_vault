---
type: Concept
---

Reinforcement Learning Fine-Tuning (RLFT) is a technique used to improve the decision-making abilities of large language models (LLMs) by fine-tuning them on self-generated Chain-of-Thought (CoT) rationales. This process involves fine-tuning the model to iteratively refine its reasoning process, favoring CoT patterns and actions that lead to higher rewards. RLFT has been shown to enhance exploration and narrow the knowing-doing gap in LLMs, making them more effective in decision-making scenarios.