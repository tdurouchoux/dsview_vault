---
type: Concept
---

Knowledge Distillation is a technique where a smaller model (student) is trained to mimic the behavior of a larger model (teacher). This process involves transferring knowledge from the larger model to the smaller one, resulting in a more efficient model with similar performance. Codistillation is a training technique used to transfer knowledge from a larger, more powerful model (teacher model) to smaller models (student models). In the context of Llama 4, codistillation involves using Llama 4 Behemoth as the teacher model to improve the performance of smaller models like Llama 4 Scout and Llama 4 Maverick. This process involves dynamically weighting soft and hard targets through training, which helps to amortize the computational cost of resource-intensive forward passes. Codistillation is particularly effective in enhancing the quality of smaller models by leveraging the advanced capabilities of the teacher model. Torchtune supports knowledge distillation for various models. Distillation in the context of LLMs refers to the process of training smaller models on the outputs of larger, more capable models. This involves using the larger model to generate a dataset of responses, which are then used to fine-tune smaller models. Distillation aims to create smaller, more efficient models that retain much of the reasoning capability of the larger models, making them more accessible and cost-effective to deploy.