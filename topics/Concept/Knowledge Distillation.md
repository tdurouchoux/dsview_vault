---
type: Concept
---

Knowledge Distillation is a technique where a smaller, more efficient model (student model) is trained to mimic the behavior of a larger, more complex model (teacher model). This process involves transferring the knowledge from the teacher model to the student model, resulting in a smaller model that can perform almost as well as the larger model. Distillation in the context of machine learning refers to the process of transferring knowledge from a large, complex model (teacher) to a smaller, simpler model (student). The goal is to create a smaller model that can perform almost as well as the larger model, making it more suitable for deployment in resource-constrained environments. The teacher model generates soft targets (probability distributions over classes) for the training data, which the student model uses to learn. This process can involve multiple rounds of training, where the student model is refined using the soft targets. Distillation can improve the efficiency of the student model without significantly sacrificing accuracy, making it a valuable technique for model compression and deployment in edge devices. In the context of LLMs, distillation involves fine-tuning smaller models on the outputs generated by larger models, such as those trained with supervised fine-tuning (SFT) and reinforcement learning (RL). This process aims to create smaller models that retain much of the performance of the larger models while being more cost-effective and suitable for deployment on lower-end hardware. In recommendation systems, knowledge distillation involves transferring knowledge from a larger, more complex teacher model to a smaller, more efficient student model. This process helps improve the performance and efficiency of recommendation models by leveraging the insights and patterns learned by the teacher model. Techniques such as auxiliary distillation and self-auxiliary distillation are used to enhance the effectiveness of knowledge transfer.