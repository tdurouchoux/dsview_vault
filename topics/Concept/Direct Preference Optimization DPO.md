---
type: Concept
---

Direct Preference Optimization (DPO) is a method used to align language models with human preferences by optimizing them directly on preference data. This approach involves fine-tuning the model using feedback data to improve its performance and alignment with desired outcomes. In the development of SmolLM2, DPO was applied using UltraFeedback to enhance the model's instruction-following capabilities and overall performance.