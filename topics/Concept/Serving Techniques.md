---
type: Concept
---

Serving Techniques for Mixture of Experts (MoE) models involve methods to deploy and run these models efficiently in production environments. Techniques such as distillation, routing full sentences or tasks to experts, and aggregation of experts are used to reduce the number of parameters and improve inference speed, making MoE models more practical for real-world applications.