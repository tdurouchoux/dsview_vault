---
type: Concept
---

A mechanism used in the decoder of the Transformer architecture to connect the decoder's output to the encoder's output. Cross-attention allows the decoder to attend to the encoder's output, enabling it to generate sequences that are conditioned on the input sequence. This mechanism is crucial for tasks such as machine translation, where the output sequence is generated based on the input sequence.