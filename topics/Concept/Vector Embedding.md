---
type: Concept
---

Vector embedding is a technique used in machine learning and natural language processing to represent data, such as words or phrases, as dense vectors in a high-dimensional space. The goal of vector embedding is to capture the semantic meaning of the data in a continuous vector form, where semantically similar items are mapped to nearby points in the vector space. This approach enables efficient and effective processing of data in various tasks, such as text classification, information retrieval, recommendation systems, similarity search, clustering, and semantic text splitting. Vector embeddings are typically learned using neural networks, such as autoencoders or word2vec, which map the input data to the embedding space and optimize the embedding vectors to minimize a loss function that measures the quality of the embeddings. They are used to encode complex relationships between entities, such as words, products, or people, and are often high-dimensional, making them useful for encoding rich information. In the context of transformers, embeddings are used to convert input tokens into vectors that can be processed by the model. These embeddings are updated through the attention mechanism to incorporate contextual information, allowing the model to understand and generate more nuanced and contextually appropriate outputs. They are essential for various NLP tasks, including similarity computation, ranking, clustering, and semantic text splitting, and are derived from large language models (LLMs).