---
type: Concept
---

Supervised Fine-Tuning (SFT) is a process used to fine-tune base models on diverse datasets to improve their performance on specific tasks. In the context of TranslateGemma, SFT involves fine-tuning the Gemma 3 models on a rich mix of human-translated texts and high-quality synthetic translations generated by state-of-the-art models. This process achieves broad language coverage and high fidelity, even in low-resource languages. Supervised Fine-Tuning (SFT) is also a method used to adapt pre-trained language models to specific tasks by fine-tuning them on a labeled dataset. This process helps the model to better understand and generate responses that are more accurate and relevant to the task at hand. In ZeroSearch, SFT is used to transform the LLM into a retrieval module capable of generating both useful and noisy documents in response to queries. This process involves training the model on input-output pairs to improve its performance on particular tasks. In the context of SmolLM2, SFT was used to develop the instruct version of the model, leveraging both public datasets and curated datasets to enhance its capabilities in tasks such as text rewriting, summarization, and function calling.