---
type: Concept
---

A GPU (Graphics Processing Unit) is a specialized processor originally designed for rendering graphics but now widely used for parallel processing tasks, including machine learning and data science. Modern GPUs, like the NVIDIA H100 and B200, are optimized for matrix multiplications and other compute-intensive operations, making them suitable for training large-scale models. GPUs consist of multiple Streaming Multiprocessors (SMs), each containing Tensor Cores for matrix operations and CUDA cores for general-purpose computing. They also feature a hierarchy of memory, including registers, shared memory (SMEM), L2 cache, and high-bandwidth memory (HBM), which are crucial for efficient data processing.