---
type: Concept
---

Tokens are the basic units into which text is divided for processing by language models. Embeddings are vector representations of these tokens, capturing their semantic meaning in a continuous vector space. These embeddings allow language models to understand and generate human-like text by leveraging the relationships between words and phrases.