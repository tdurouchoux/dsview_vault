---
type: Concept
---

Tokens and embeddings are fundamental concepts in natural language processing and large language models. Tokens are the smallest units of meaning in a language, such as words or subwords, that are used as input to a model. Embeddings are numerical representations of these tokens in a high-dimensional space, capturing semantic and syntactic relationships. These embeddings allow the model to understand and generate human-like text by transforming tokens into vectors that the model can process and manipulate. Embeddings are a way of representing entities as points in N-dimensional space, where similar or related entities are placed close to each other. They are used to encode complex relationships between entities, such as words, products, or people, and are often high-dimensional, making them suitable for encoding rich information. Embeddings capture semantic meanings and relationships between tokens. In transformers, embeddings are updated through the attention mechanism to incorporate contextual information, enhancing the model's understanding of the input data.