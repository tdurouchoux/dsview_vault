---
type: Concept
---

MetaP is a new training technique developed for setting critical model hyper-parameters such as per-layer learning rates and initialization scales. This technique allows for reliable hyper-parameter tuning that transfers well across different values of batch size, model width, depth, and training tokens. MetaP is used in the training of Llama 4 models to ensure optimal performance and efficiency. By using MetaP, the models can achieve high quality with a fixed training FLOPs budget, making it a valuable tool in the development of advanced AI models.