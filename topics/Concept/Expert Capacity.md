---
type: Concept
---

Expert Capacity in Mixture of Experts (MoE) models refers to the threshold that limits the number of tokens an expert can process. This concept is crucial for managing the load on experts and ensuring efficient training and inference. By setting an appropriate capacity factor, the model can balance the load across experts and optimize communication costs.