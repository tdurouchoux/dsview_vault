---
type: Concept
---

AI Inference refers to the process of using a trained machine learning model to make predictions or generate outputs from new data. Unlike training, which involves learning from data, inference is the phase where the model is deployed to perform tasks such as answering questions, generating text, or creating images. Inference is a critical part of AI applications, as it directly impacts the user experience and the efficiency of AI systems. The energy consumption during inference is a significant consideration, as it can account for a large portion of the overall energy use of AI models.