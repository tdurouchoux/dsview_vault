---
type: Concept
---

Open LLM datasets for pre-training are collections of text data used to train large language models. These datasets are designed to be freely available and often include a diverse range of text sources to ensure the models learn a wide variety of language patterns. Examples include the RedPajama dataset, which aims to reproduce the training dataset of over 1.2 trillion tokens used for models like LLaMA.