---
type: Concept
---

Transformer-based Language Models (LLMs) are a type of neural network architecture designed for natural language processing tasks. They use self-attention mechanisms to weigh the importance of input data, making them highly effective for understanding and generating human language. These models are foundational in modern AI applications, including text generation, translation, and semantic search.