---
type: Concept
---

Feature interpretability refers to the ability to understand and explain the features extracted from a model. In the context of the provided text, assessing feature interpretability involves investigating whether the features extracted by sparse autoencoders are actually interpretable and explain model behavior. This includes looking at a handful of relatively straightforward features and providing evidence that they are interpretable, as well as demonstrating that more complex features track very abstract concepts. Automated interpretability methods are used to evaluate a larger number of features and compare them to neurons, ensuring that the features are not only interpretable but also causally relevant to the model's behavior.