---
type: Concept
---

The BLEU (Bilingual Evaluation Understudy) score is a metric used to evaluate the quality of machine-translated text by comparing it to one or more human-generated reference translations. It measures the precision of n-grams (contiguous sequences of n items from a given sample of text) in the candidate translation against the reference translations. The BLEU score ranges from 0 to 1, where a higher score indicates better quality. It is commonly used in machine translation tasks but can also be applied to other text generation tasks such as text summarization and paraphrase generation. The BLEU score does not consider grammatical correctness or the overall meaning of the sentences, focusing solely on the precision of n-grams.