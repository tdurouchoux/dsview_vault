---
type: Concept
---

SwiGLU (Gated Linear Unit) is an activation function used in neural networks, particularly in transformer models. It combines the benefits of gating mechanisms with linear units to improve the model's performance. SwiGLU helps in controlling the flow of information through the network, leading to better learning and generalization. It is often used in place of traditional activation functions like ReLU to enhance model performance.