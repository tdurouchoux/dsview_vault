---
type: Concept
---

Kullback-Leibler (KL) divergence is a measure of the difference between two probability distributions. In the context of RLHF, KL divergence is used as a penalty term in the reward function to prevent the language model from deviating too much from the initial pretrained model. This helps to ensure that the fine-tuned model generates coherent and reasonable text.