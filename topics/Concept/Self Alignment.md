---
type: Concept
---

Self-Alignment in the context of LLMs refers to the process where a language model improves its own performance and alignment with human intentions without extensive human intervention. This involves techniques such as self-instruction, self-rewarding, and self-play, where the model generates its own training data, evaluates its outputs, and iteratively improves. Self-alignment aims to create models that can follow complex instructions, generate high-quality outputs, and align better with user needs and ethical guidelines.