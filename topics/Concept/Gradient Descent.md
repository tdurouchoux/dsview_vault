---
type: Concept
---

Gradient descent is an optimization algorithm used to minimize the loss function in machine learning and deep learning. It adjusts the parameters iteratively by moving in the direction of the steepest descent as defined by the negative of the gradient. The step size is determined by the learning rate (Î±). Gradient descent can be slow in converging, especially in regions with pathological curvature, where the landscape is not scaled properly, leading to slow progress in certain directions.