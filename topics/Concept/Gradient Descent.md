---
type: Concept
---

Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning models. It adjusts the parameters iteratively in the opposite direction of the gradient of the loss function with respect to the parameter. The step size, or learning rate, determines the size of the steps taken towards the minimum. The algorithm can be slow and may get stuck in local minima or saddle points, especially in complex landscapes with pathological curvature.