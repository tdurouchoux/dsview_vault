---
type: Concept
---

Preference alignment and tuning refer to techniques like DPO (Direct Preference Optimization) and ORPO (Offline Reinforcement Learning from Human Preferences) used to align language models with human preferences, making their outputs more desirable and relevant. This involves fine-tuning the model on data that reflects user preferences, ensuring that the model generates outputs that are more aligned with what users find desirable or appropriate.