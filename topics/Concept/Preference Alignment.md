---
type: Concept
---

Preference alignment is a post-training process aimed at aligning the outputs of a language model with human preferences. This involves training the model on data that reflects human preferences, such as comparing different responses to the same prompt and selecting the preferred one. The goal is to improve the model's ability to generate outputs that are more aligned with human values and preferences. It involves techniques like Direct Preference Optimization (DPO), GRPO, and PPO to reduce toxicity, hallucinations, and improve the usefulness and tone of the model's responses.