---
type: Concept
---

LLM-as-a-judge is a technique used for evaluating the quality of text-to-SQL outputs and the performance of large language models (LLMs) by using a stronger LLM to assess the outputs of weaker LLMs. It involves using a large language model to assess the performance of another model or system, including presenting the stronger LLM with pairs of outputs and asking it to select the better one, or using chain-of-thought prompting to improve the reliability of the evaluation. This technique is particularly useful for evaluating performance on ambiguous and unclear tasks, and it can reduce the cost of evaluation while still providing valuable insights into system performance and identifying areas for improvement.