---
type: Concept
---

A Vision-Language-Action (VLA) model integrates visual, linguistic, and action modalities to enable robots to understand and interact with their environment. These models process images, text, and actions, allowing robots to perform tasks that require a combination of perception, language understanding, and physical manipulation.