---
type: Concept
---

Attention mechanisms are a key component of transformer-based models, allowing them to focus on different parts of the input sequence when producing each part of the output sequence. Self-attention, in particular, enables the model to weigh the importance of different input elements dynamically, capturing long-range dependencies and context.