---
type: Concept
---

Gradient Boosted Decision Trees (GBDTs) are a type of ensemble learning method used for regression and classification tasks. They build a strong predictive model by combining the predictions of multiple weak models, typically decision trees. The models are built sequentially, with each new tree correcting the errors of the previous ones. In the context of the provided text, GBDTs are mentioned as part of diverse baselines and ensembling techniques. The text highlights the use of GPU-accelerated GBDTs, such as XGBoost, LightGBM, and CatBoost, which allow for faster training and experimentation. These tools are practical for handling large-scale datasets and improving model performance through rapid iteration and validation. Gradient Boosting Machines (GBMs) are a type of ensemble learning technique used for regression and classification tasks. They build models in a stage-wise fashion, where each new model attempts to correct the errors of the previous one. Popular implementations include LightGBM, XGBoost, and CatBoost, known for their efficiency, speed, and ability to handle large datasets with high dimensionality. They are particularly effective in time series forecasting due to their ability to handle many features, require little data preprocessing, and provide fast training times.