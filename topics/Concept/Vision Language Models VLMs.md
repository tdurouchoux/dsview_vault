---
type: Concept
---

Vision Language Models (VLMs) are multimodal models that combine visual and linguistic data to perform tasks that require understanding and processing both types of information, such as image captioning, visual question answering, and document understanding by leveraging their ability to process and integrate multi-modal data. These models are particularly useful in applications where both visual and textual data need to be analyzed together, including enhancing the quality of data extraction from documents.