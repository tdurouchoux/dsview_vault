---
type: Concept
---

Multi-head attention is an advanced attention mechanism used in the Transformer model to enhance its ability to focus on different parts of the input sequence simultaneously. Multi-headed attention is an extension of the attention mechanism where multiple attention heads operate in parallel. Each head has its own set of query, key, and value matrices, allowing the model to capture different types of relationships and contextual information simultaneously. This enhances the model's ability to understand complex patterns in the data. Instead of performing a single attention function, multi-head attention linearly projects the queries, keys, and values multiple times with different learned linear projections. This allows the model to jointly attend to information from different representation subspaces at different positions, improving its ability to capture complex patterns and relationships in the data. Multi-head attention consists of several attention layers running in parallel, each with its own set of queries, keys, and values. The outputs of these attention layers are concatenated and projected to produce the final values.