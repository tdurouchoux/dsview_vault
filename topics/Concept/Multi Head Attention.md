---
type: Concept
---

Multi-head attention is an extension of the attention mechanism that allows the model to focus on different positions or representation subspaces simultaneously. It involves running multiple attention mechanisms (heads) in parallel, each with its own set of query, key, and value matrices. This approach enables the model to jointly attend to information from different parts of the sequence, improving its ability to capture complex patterns and relationships in the data.