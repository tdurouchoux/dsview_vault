---
type: Concept
---

G-Eval is a framework for evaluating the quality of generated text using large language models (LLMs). It provides a reference-free method to assess various aspects of text quality, such as relevance, coherence, consistency, and fluency. G-Eval leverages the capabilities of LLMs to generate scores for these criteria, offering a more nuanced and context-aware evaluation compared to traditional metrics like ROUGE or BERTScore. This approach is particularly useful for tasks where reference texts are scarce or unavailable, enabling more flexible and scalable evaluation of text generation systems.