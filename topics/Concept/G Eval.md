---
type: Concept
---

G-Eval is a framework for evaluating the quality of generated text using large language models (LLMs). It leverages the capabilities of LLMs to assess various aspects of text quality, such as relevance, coherence, consistency, and fluency, without relying on reference texts. G-Eval provides a reference-free evaluation method, making it applicable to scenarios where human-generated references are scarce or unavailable. This approach allows for more flexible and scalable evaluation of text generation tasks.