---
type: Concept
---

Prompt Engineering is a technique used to guide the output of large language models (LLMs) by providing specific instructions or examples. It involves crafting inputs that elicit desired responses from the model. Effective Prompt Engineering can significantly improve the quality and relevance of the model's outputs. Techniques include few-shot learning, chain-of-thought prompting, and providing relevant resources to reduce hallucinations and increase trust. Prompt optimization involves improving the prompts used to interact with language models to achieve better and more accurate results. This can include rephrasing, using examples, and leveraging techniques like few-shot learning. DSPy provides tools to automate and optimize prompts based on defined metrics and training data. Prompt optimization is the process of systematically improving prompts to enhance the performance of large language models (LLMs). It involves using various techniques to refine prompts based on real data, aiming to close the gap between the user's intent and the model's instructions. This process can significantly increase accuracy and effectiveness, especially in tasks where the model lacks domain knowledge. Prompt optimization can be thought of as a form of long-term memory, allowing the model to adapt directly from the data.