---
type: Concept
---

Model Quantization is a technique used to reduce the precision of model weights, which shrinks the model size and its memory requirements without significantly compromising output quality. This technique improves inference throughput and reduces costs, making it particularly useful for self-hosted models. In the context of bitnet.cpp, quantization types like I2_S and TL1 are used to optimize the inference process for 1-bit LLMs.