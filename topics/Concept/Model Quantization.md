---
type: Concept
---

Model Quantization is a technique used to reduce the size and computational requirements of machine learning models by decreasing the precision of the model's weights and activations. This process can significantly improve the model's inference speed and reduce memory usage, making it more efficient to deploy and run on resource-constrained devices or environments. It typically involves reducing the precision from floating-point to lower-bit representations like 8-bit integers.