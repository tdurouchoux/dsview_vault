---
type: Concept
---

Vision language models are multimodal models that combine computer vision and natural language processing capabilities. These models can understand and generate text based on visual inputs, enabling applications like image captioning, visual question answering, and more. Vision-Language Models (VLMs) are AI models that combine computer vision and natural language processing to understand and generate content based on both visual and textual inputs. These models are capable of performing tasks such as image captioning, visual question answering, and content extraction from complex documents. VLMs enhance the accuracy and quality of data extraction by leveraging their multimodal capabilities.