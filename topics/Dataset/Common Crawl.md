---
type: Dataset
---

Common Crawl is a large-scale open repository of web crawl data, containing over 300 billion web pages spanning 15 years. It is maintained as a free and open corpus since 2007 and is cited in over 10,000 research papers. The dataset is updated monthly with 3â€“5 billion new pages and is used for wholesale extraction, transformation, and analysis of open web data by researchers. Common Crawl is a non-profit organization that crawls the web and makes its data publicly available. It provides a vast repository of web data, including HTML, text, and other content, which can be used for various research and development purposes. Common Crawl data is used in the creation of the FineWeb dataset.