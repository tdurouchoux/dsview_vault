---
type: Dataset
---

Common Crawl is a non-profit organization that maintains an open repository of web crawl data. It provides a vast corpus of web data, including over 300 billion web pages spanning 15 years. This dataset is freely available and has been cited in over 10,000 research papers. It is used for various research purposes, including semantic analysis, web crawling studies, computational analysis, training machine learning models, data mining, and web archiving. The dataset is updated monthly with 3â€“5 billion new pages.