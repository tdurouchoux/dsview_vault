---
type: Dataset
---

The Massive Text Embedding Benchmark (MTEB) is a comprehensive benchmark used to evaluate and compare text embedding models across diverse tasks such as retrieval and classification. It provides a mean task score to rank models, offering a standardized way to assess their performance and capabilities. MTEB includes 56 datasets across 8 tasks and supports up to 112 different languages. It provides a leaderboard to compare models and is extensible, allowing contributions of new tasks, datasets, and metrics. WordLlama has been evaluated on the MTEB benchmarks and has shown significant improvements over traditional word models like GloVe 300d.