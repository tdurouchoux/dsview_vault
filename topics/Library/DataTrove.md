---
type: Library
---

A large-scale data processing library designed for handling and processing extensive datasets efficiently. It is used in the creation of the FineWeb dataset to filter, clean, and deduplicate web data from CommonCrawl, ensuring high-quality data for training large language models. DataTrove is a library designed for processing, filtering, and deduplicating text data at a large scale. It offers prebuilt processing blocks and a framework for adding custom functionality. The library supports various file systems through fsspec and is optimized for low memory usage, making it suitable for large workloads such as processing training data for large language models (LLMs). DataTrove pipelines are platform-agnostic, running locally or on slurm clusters, and include features for logging, data management, and custom block creation.