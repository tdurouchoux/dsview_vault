---
type: Library
---

vLLM is a fast and easy-to-use library for LLM inference and serving. It is designed to provide state-of-the-art serving throughput, efficient management of attention key and value memory with PagedAttention, and continuous batching of incoming requests. vLLM supports a wide range of hardware including NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs, GaudiÂ® accelerators and GPUs, IBM Power CPUs, TPU, and AWS Trainium and Inferentia Accelerators. It also offers prefix caching support, multi-LoRA support, and seamless integration with popular HuggingFace models. vLLM is developed in the Sky Computing Lab at UC Berkeley and has evolved into a community-driven project with contributions from both academia and industry.