---
type: Library
---

vLLM is a fast and easy-to-use library for LLM inference and serving. It is designed to provide state-of-the-art serving throughput, efficient memory management with PagedAttention, and continuous batching of incoming requests. vLLM supports various decoding algorithms, including parallel sampling, beam search, and more. It also offers seamless integration with popular HuggingFace models and supports a wide range of hardware, including NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, and TPU. Additionally, vLLM supports diverse hardware plugins such as Intel Gaudi, IBM Spyre, and Huawei Ascend. It provides prefix caching support and multi-LoRA support, making it a versatile tool for LLM inference and serving.