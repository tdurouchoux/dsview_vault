---
type: Tool
---

A tool designed for semantic deduplication and dataset management. It helps in removing semantically similar documents from a corpus, ensuring high-quality data for training models like Large Language Models (LLMs). It operates by encoding documents using a model2vec encoder and storing them in a vicinity vector store backed by usearch. This allows for fast and efficient deduplication, even for large datasets. semhash can be used to check for overlaps between train and test sets, ensuring no performance overestimation, and can also provide a birdâ€™s eye view of larger datasets by identifying approximate duplicates. Deduplication is the process of removing duplicate data from a dataset to ensure that each unique piece of information is only represented once. In the context of the FineWeb dataset, deduplication is performed on individual CommonCrawl dumps to improve the quality and performance of the dataset for training large language models.