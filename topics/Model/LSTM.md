---
type: Model
---

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) architecture capable of learning long-term dependencies. They are particularly useful for sequence prediction problems, such as time series forecasting. LSTMs have a memory cell that can maintain information in memory for long periods of time, and they have gates that regulate the flow of information into and out of the cell, allowing them to learn which data in a sequence is important to keep or discard. LSTMs are designed to overcome the vanishing gradient problem, making them suitable for learning long-term dependencies in sequential data.