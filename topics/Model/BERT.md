---
type: Model
---

BERT, or Bidirectional Encoder Representations from Transformers, is a language representation model designed to pre-train deep bidirectional representations from unlabeled text. It jointly conditions on both left and right context in all layers, allowing the pre-trained BERT model to be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.