---
type: Model
---

BERT, or Bidirectional Encoder Representations from Transformers, is a language representation model designed to pre-train deep bidirectional representations from unlabeled text. It jointly conditions on both left and right context in all layers, allowing it to be fine-tuned with minimal task-specific architecture modifications. BERT achieves state-of-the-art results on a wide range of natural language processing tasks, such as question answering and language inference.