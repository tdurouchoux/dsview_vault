---
type: Model
---

CLIP (Contrastive Language–Image Pretraining) is a neural network model developed by OpenAI that connects text and images using contrastive learning. It is trained on a dataset of images and their corresponding text descriptions. The model is capable of understanding the relationship between text and images, and it can be used to generate text descriptions for images, or to generate images from text descriptions. CLIP is a combination of an image encoder and a text encoder. Its training process can be simplified to thinking of taking an image and its caption. We encode them both with the image and text encoders respectively. We then compare the resulting embeddings using cosine similarity. When we begin the training process, the similarity will be low, even if the text describes the image correctly. We update the two models so that the next time we embed them, the resulting embeddings are similar. By repeating this across the dataset and with large batch sizes, we end up with the encoders being able to produce embeddings where an image of a dog and the sentence “a picture of a dog” are similar. Just like in word2vec, the training process also needs to include negative examples of images and captions that don’t match, and the model needs to assign them low similarity scores. OpenCLIP is a text encoder developed by LAION with support from Stability AI. It is used to improve the quality of generated images in text-to-image models. OpenCLIP is designed to enhance the understanding and generation of images based on textual descriptions, leading to more accurate and high-quality outputs.