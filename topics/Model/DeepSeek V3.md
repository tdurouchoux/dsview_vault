---
type: Model
---

DeepSeek-V3 is a strong Mixture-of-Experts (MoE) language model with 671 billion total parameters, designed for efficient inference and cost-effective training. It employs Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, along with an auxiliary-loss-free strategy for load balancing. The model is pre-trained on 14.8 trillion tokens and undergoes Supervised Fine-Tuning and Reinforcement Learning stages. It is noted for its high performance, stability, and low training cost, requiring only 2.788M H800 GPU hours for full training.