---
type: Model
---

Phi is a family of Transformer-based large language models developed by Microsoft. Phi-1 is a 1.3 billion parameters model for code, trained for 4 days on 8 A100 GPUs using a combination of "textbook quality" data from the web (6 billion tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1 billion tokens). Despite its small scale, phi-1 achieves a pass@1 accuracy of 50.6% on HumanEval and 55.5% on MBPP, demonstrating strong performance in code generation tasks. Phi-4 is a 14B parameter state-of-the-art small language model (SLM) developed by Microsoft. It excels at complex reasoning tasks, particularly in math, and is part of the Phi family of small language models. Phi-4 is designed to offer high-quality results while maintaining a smaller size compared to other models. It is available on Azure AI Foundry and Hugging Face.