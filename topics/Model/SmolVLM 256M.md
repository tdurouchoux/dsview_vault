---
type: Model
---

SmolVLM-256M is a Vision Language Model (VLM) with 256 million parameters, making it the smallest VLM ever released. Despite its compact size, it is capable of performing various multimodal tasks such as image captioning, document question answering, and basic visual reasoning. The model is designed for efficiency and can be used on constrained devices, consumer laptops, or even for browser-based inference. It is also optimized for tasks like document understanding and image captioning, making it versatile for different applications.