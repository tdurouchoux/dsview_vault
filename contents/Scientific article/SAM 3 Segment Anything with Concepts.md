---
already_read: false
link: https://arxiv.org/html/2511.16719v1
read_priority: 3
relevance: 0
source: The Batch
tags:
- Computer_Vision
- Deep_Learning
type: Content
upload_date: '2026-01-04'
---

https://arxiv.org/html/2511.16719v1
## Summary

SAM 3 introduces a unified model for detecting, segmenting, and tracking objects in images and videos based on concept prompts, which can be short noun phrases, image exemplars, or a combination of both. The model consists of an image-level detector and a memory-based video tracker that share a single backbone, with a presence head to boost detection accuracy. The model is trained on a large, diverse dataset generated by a human- and AI-in-the-loop data engine, which significantly improves annotation throughput and label diversity.

Key technical points and takeaways:

1. **Promptable Concept Segmentation (PCS)**: SAM 3 formalizes the PCS task, which takes text and/or image exemplars as input and predicts instance and semantic masks for every single object matching the concept, while preserving object identities across video frames.

2. **Model Architecture**: The model consists of a detector and a tracker that share a vision encoder. The detector is a DETR-based model conditioned on text, geometry, and image exemplars. The tracker inherits the SAM 2 transformer encoder-decoder architecture, supporting video segmentation and interactive refinement.

3. **Presence Token**: A learned global presence token is introduced to decouple recognition and localization, improving detection accuracy, especially when training with challenging negative phrases.

4. **Data Engine**: A scalable data engine is built to produce high-quality training data with 4M unique concept labels, including hard negatives, across images and videos. The data engine uses a combination of human and AI annotators to improve annotation throughput and label diversity.

5. **Segment Anything with Concepts (SA-Co) Dataset**: A new dataset is created for the PCS task, containing 207K unique concepts with exhaustive masks in 120K images and 1.7K videos, which is over 50 times more concepts than existing benchmarks.

6. **Experiments**: SAM 3 sets a new state-of-the-art in promptable segmentation, improving upon previous SAM capabilities on visual segmentation tasks and surpassing baselines on the new SA-Co benchmark by at least 2 times.

7. **Limitations**: The model struggles to generalize to out-of-domain terms, which could be mitigated by automatic domain expansion but requires extra training. Other limitations are discussed in the appendix.

The model and the SA-Co benchmark are open-sourced to facilitate future research and applications in computer vision.
## Links

- [SAM 3 GitHub Repository](https://github.com/facebookresearch/sam3) : The GitHub repository for SAM 3, providing access to the code and resources for the Segment Anything Model 3.
- [SAM 3 Demo](https://segment-anything.com) : A demo of the Segment Anything Model 3, showcasing its capabilities in segmenting objects in images and videos based on concept prompts.
- [SAM 3 Website](https://ai.meta.com/sam3) : The official website for SAM 3, providing detailed information about the model, its features, and applications.

## Topics

![[topics/Model/Segment Anything Model SAM 3]]

![[topics/Concept/Promptable Concept Segmentation PCS]]

![[topics/Concept/Presence Token]]

![[topics/Concept/Data Engine]]

![[topics/Dataset/Segment Anything with Concepts SA Co Dataset]]