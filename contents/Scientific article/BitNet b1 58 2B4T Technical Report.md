---
already_read: false
link: https://arxiv.org/abs/2504.12285
read_priority: 3
relevance: 0
source: Data Points
tags:
- Large_Language_Model
type: Content
upload_date: '2025-04-21'
---

https://arxiv.org/abs/2504.12285
## Summary

BitNet b1.58 2B4T is an open-source, native 1-bit Large Language Model (LLM) with 2 billion parameters, trained on a 4 trillion token corpus. It demonstrates performance comparable to leading open-weight, full-precision LLMs of similar size, with significant computational efficiency advantages, including reduced memory footprint, energy consumption, and decoding latency. The model weights and open-source inference implementations for GPU and CPU architectures are available on Hugging Face. The work is ongoing, with evaluations covering language understanding, mathematical reasoning, coding proficiency, and conversational ability.
## Links

- [BitNet b1.58 2B4T Technical Report PDF](https://arxiv.org/abs/2504.12285) : PDF version of the BitNet b1.58 2B4T Technical Report.
- [Hugging Face](https://huggingface.co/huggingface) : Hugging Face platform where the model weights and inference implementations are released.
- [Litmaps](https://www.litmaps.co/) : Litmaps tool for visualizing the context and connections between research papers.
- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2504.12285) : Semantic Scholar page for the paper, providing additional metadata and insights.
- [Papers With Code](https://paperswithcode.com/) : Papers With Code platform that may include implementations and code related to the research.

## Topics

![](topics/Model/BitNet%20b1%2058)