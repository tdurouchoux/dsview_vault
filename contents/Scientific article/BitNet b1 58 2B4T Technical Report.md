---
already_read: false
link: https://arxiv.org/abs/2504.12285
read_priority: 3
relevance: 0
source: Data Points
tags:
- Large_Language_Model
type: Content
upload_date: '2025-04-21'
---

https://arxiv.org/abs/2504.12285
## Summary

BitNet b1.58 2B4T is an open-source, 1-bit Large Language Model (LLM) with 2 billion parameters. It was trained on a massive corpus of 4 trillion tokens and evaluated across various benchmarks, including language understanding, mathematical reasoning, coding, and conversation. The model performs comparably to leading open-weight, full-precision LLMs of similar size but offers significant computational efficiency benefits, such as reduced memory usage, energy consumption, and decoding latency. The model weights and inference implementations for both GPU and CPU are available on Hugging Face to encourage further research and adoption. The work is still in progress.
## Links

- [BitNet b1.58 2B4T Technical Report PDF](https://arxiv.org/abs/2504.12285) : PDF version of the BitNet b1.58 2B4T Technical Report.
- [Hugging Face Documentation](https://huggingface.co/docs/hub/spaces) : Documentation for Hugging Face, where the model weights are released.

## Topics

![](topics/Model/BitNet%20b1%2058)