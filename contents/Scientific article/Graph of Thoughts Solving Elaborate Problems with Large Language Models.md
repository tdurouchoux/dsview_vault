---
already_read: false
link: https://arxiv.org/pdf/2308.09687.pdf
read_priority: 1
relevance: 0
source: null
tags:
- Large_Language_Model
- AI_agent
type: Content
upload_date: '2023-09-14'
---

https://arxiv.org/pdf/2308.09687.pdf
## Summary

The paper introduces Graph of Thoughts (GoT), a framework that enhances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea is to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. The paper illustrates that GoT offers advantages over state-of-the-art on different tasks, for example, increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. The paper ensures that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. The paper brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.
## Links


## Topics

![](topics/Concept/Graph%20of%20Thoughts%20GoT)

![](topics/Concept/Chain%20of%20Thought%20Prompting)

![](topics/Concept/Tree%20of%20Thoughts%20ToT)

![](topics/Concept/Graph%20Databases)

![](topics/Concept/Graph%20Mining)

![](topics/Concept/Graph%20Pattern%20Matching)

![](topics/Concept/Graph%20Streaming)

![](topics/Concept/Large%20Language%20Models%20LLMs)

![](topics/Concept/Prompt%20Engineering)

![](topics/Concept/Graph%20Neural%20Networks)