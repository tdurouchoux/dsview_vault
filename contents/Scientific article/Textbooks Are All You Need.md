---
already_read: false
link: https://arxiv.org/pdf/2306.11644
read_priority: 3
relevance: 0
source: null
tags:
- Large_Language_Model
- Natural_Language_Processing
type: Content
upload_date: '2025-01-28'
---

https://arxiv.org/pdf/2306.11644
## Summary

The paper introduces phi-1, a Transformer-based language model for code with 1.3B parameters, trained on a high-quality dataset of 7B tokens, including synthetically generated textbooks and exercises. Despite its smaller size, phi-1 achieves competitive performance on coding benchmarks like HumanEval and MBPP, outperforming larger models in some cases. The model's success is attributed to the quality of its training data, which is designed to be clear, self-contained, and instructive, similar to a textbook. The paper also discusses the importance of data quality in training language models and the potential for high-quality data to improve model performance and reduce environmental costs. Additionally, the paper explores the model's emergent properties and its ability to generalize to tasks not explicitly present in the finetuning dataset. The authors release the model for community evaluation and discuss potential limitations and future directions for research.
## Links


## Topics

![](topics/Model/Phi)

![](topics/Dataset/CodeTextbook)

![](topics/Dataset/CodeExercises)

![](topics/Concept/Emergent%20Properties)

![](topics/Concept/Data%20Pruning)

![](topics/Concept/Scaling%20Laws)