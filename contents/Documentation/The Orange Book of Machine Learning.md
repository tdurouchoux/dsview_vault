---
already_read: false
link: https://carl-mcbride-ellis.github.io/TOBoML/TOBoML.pdf
read_priority: 1
relevance: 0
source: null
tags:
- Statistics
- Data_Analysis
- Data_Quality
type: Content
upload_date: '2024-07-10'
---

https://carl-mcbride-ellis.github.io/TOBoML/TOBoML.pdf
## Summary

The Orange Book of Machine Learning (free version) is a comprehensive guide to making predictions using supervised regression and classification for tabular data. The book is structured into several chapters, each focusing on different aspects of machine learning. Here is a concise summary of the main technical points and key takeaways:

1. **Introduction**:
   - Discusses the fundamental assumption that data is composed of signal and noise.
   - Introduces the concepts of interpolation and curve fitting.
   - Explains errors and residuals, and the sources of uncertainty (aleatoric and epistemic).
   - Covers confidence and prediction intervals, and the importance of explainability and interpretability.

2. **Statistics**:
   - Covers essential statistical concepts such as centrality (mean, median, mode) and dispersion (variance, MAD, quartiles).
   - Discusses the Gaussian and Galton distributions, skewness, and kurtosis.
   - Introduces tests for normality and Chebyshev’s inequality.

3. **Exploratory Data Analysis (EDA)**:
   - Emphasizes the importance of data quality and the dimensions of data (appropriateness, readiness, reliability, sensitivity, sufficiency).
   - Covers data quality issues, data types, and descriptive statistics.
   - Discusses visualization techniques such as box plots, violin plots, raincloud plots, scatter plots, histograms, and pairplots.
   - Introduces correlation coefficients (Pearson, Spearman) and mutual information (MI).

4. **Data Cleaning**:
   - Addresses missing values (NULL and NaN), outliers, inliers, and extreme values.
   - Covers duplicated rows, Boolean columns, zero variance columns, and feature scaling (standardization and normalization).
   - Discusses categorical features (ordinal and nominal) and their encoding (label encoding, one-hot encoding, frequency encoding).

5. **Cross-Validation**:
   - Explains the importance of splitting data into training, validation, and test sets.
   - Covers cross-validation techniques (k-fold, nested cross-validation) and data leakage.
   - Discusses covariate shift and concept drift.

6. **Regression**:
   - Introduces the regression baseline model and univariate linear regression.
   - Covers calculating β1 and β0, ordinary least squares, and the normal equation.
   - Discusses polynomial regression, extrapolation, and explainability.
   - Introduces the loss and cost functions, gradient descent, and metrics (RMSE, MAE, R2).
   - Covers the decision tree regressor, overfitting, quantile regression, and conformal prediction intervals.

7. **Classification**:
   - Introduces the classification baseline model and logistic regression.
   - Covers the log-loss function, decision tree classifier, and classification metrics (accuracy, confusion matrix, precision, recall, F1 score, AUC ROC).
   - Discusses imbalanced classification, overfitting, the no free lunch theorem, and classifier calibration.

8. **Ensemble Estimators**:
   - Introduces the random forest, weak learners, and boosting (AdaBoost, gradient boosted decision trees).
   - Covers the convex combination of model predictions (CCMP) and stacking.

9. **Hyperparameter Optimization**:
   - Discusses grid search, random search, and halving grid search for hyperparameter optimization.

10. **Feature Engineering and Selection**:
    - Covers feature engineering (interaction and cross features, bucketing, power transforms, user-defined transforms, external secondary features).
    - Discusses feature selection (correlation, permutation importance, stepwise regression, LASSO, Boruta trick, native feature importance plots, principal component analysis).

11. **Why No Neural Networks/Deep Learning?**:
    - Discusses the limitations of neural networks for tabular data and their performance compared to other methods.

The book also includes essential reading recommendations, packages, and other resources for further learning.
## Links


## Topics

![](topics/Concept/AutoML)

![](topics/Concept/Knowledge%20Graphs)

![](topics/Concept/Reinforcement%20Learning%20from%20Human%20Feedback%20RLHF)