---
already_read: true
link: https://github.com/Lightning-AI/LitServe
read_priority: 1
relevance: 0
source: null
tags:
- Development_tool
- MlOps
type: Content
upload_date: '2024-09-04'
---

https://github.com/Lightning-AI/LitServe
## Summary

LitServe is a tool designed for deploying AI models and systems with minimal MLOps and YAML configuration. It supports a wide range of AI applications, including agents, RAG, pipelines, and various model types (vision, audio, text). Key features include:

- **Flexibility**: Deploy any model or pipeline with full control over batching, streaming, and custom logic.
- **Performance**: Optimized for AI workloads, offering 2x faster performance than FastAPI with features like GPU autoscaling and multi-worker handling.
- **Ease of Use**: Simple setup with Python, supporting both self-hosting and one-click deployment to Lightning AI.
- **Compatibility**: Works with PyTorch, JAX, TensorFlow, and more, and is OpenAPI compliant.
- **Community and Support**: Open-source with community contributions, and enterprise support options available.

LitServe is not a direct alternative to vLLM or Ollama but provides the flexibility to build similar functionalities if needed. It includes a variety of examples and templates for common use cases.
## Links

- [LitServe Documentation](https://lightning.ai/docs/litserve) : Official documentation for LitServe, providing detailed information on features, examples, and usage.
- [LitServe Features](https://lightning.ai/docs/litserve/features) : Detailed overview of LitServe's features, including deployment options, performance, and capabilities.
- [LitServe Examples](https://lightning.ai/docs/litserve/examples) : A collection of examples demonstrating how to use LitServe for various AI model deployments and pipelines.

## Topics

![](topics/Tool/LitServe)

![](topics/Concept/Multi%20GPU%20Autoscaling)

![](topics/Concept/Retrieval%20Augmented%20Generation%20RAG)