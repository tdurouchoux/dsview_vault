---
already_read: true
link: https://commoncrawl.org/
read_priority: 0
relevance: 2
source: null
tags:
- Data_Engineering
- Data_Analysis
type: Content
upload_date: '2025-01-28'
---

https://commoncrawl.org/
## Summary

Common Crawl is a non-profit organization that maintains an open repository of web crawl data, accessible to anyone. Founded in 2007, it provides a vast corpus of over 300 billion web pages spanning 15 years, with 3-5 billion new pages added monthly. The data is used for research, cited in over 10,000 papers, and supports various use cases like semantic analysis, web crawling studies, and censorship analysis. The repository includes web graphs, crawl stats, and errata, with resources for getting started and community engagement through blogs, mailing lists, and forums. Notable research papers and projects leverage this data for tasks ranging from geolocating news articles to analyzing web graph structures and enhancing language models.
## Links

- [Analysis of Censorship on Amazon.com](https://citizenlab.ca/2024/11/analysis-of-censorship-on-amazon-com/) : Research paper analyzing censorship practices on Amazon.com, cited in the content.
- [CC Crawl Statistics](https://commoncrawl.github.io/cc-crawl-statistics/) : Statistics and data related to the Common Crawl's web crawling activities.
- [CC Webgraph Statistics](https://commoncrawl.github.io/cc-webgraph-statistics/) : Statistics and data related to the Common Crawl's web graph activities.

## Topics

![](topics/Dataset/Common%20Crawl)