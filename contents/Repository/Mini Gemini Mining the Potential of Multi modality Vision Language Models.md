---
already_read: false
link: https://mini-gemini.github.io/
read_priority: 1
relevance: 0
source: null
tags:
- Large_Language_Model
type: Content
upload_date: '2024-04-29'
---

https://mini-gemini.github.io/
## Summary

Mini-Gemini is a framework designed to enhance multi-modality Vision Language Models (VLMs). It addresses the performance gap between current VLMs and advanced models like GPT-4 and Gemini by focusing on three key aspects: high-resolution visual tokens, high-quality data, and VLM-guided generation. The framework employs dual vision encoders to provide both low and high-resolution visual embeddings, and introduces patch info mining to refine these embeddings. Additionally, it utilizes a high-quality dataset to improve image comprehension and reasoning-based generation. Mini-Gemini supports dense and Mixture of Experts (MoE) Large Language Models (LLMs) ranging from 2B to 34B parameters. It has demonstrated leading performance in several zero-shot benchmarks, even surpassing some developed private models. The framework is conceptually simple, integrating text and images for both comprehension and generation.
## Links

- [Mini-Gemini GitHub Repository](https://github.com/dvlab-research/MiniGemini) : The GitHub repository for Mini-Gemini, providing access to the source code and implementation details of the Mini-Gemini framework.
- [Mini-Gemini Paper on arXiv](https://arxiv.org/abs/2403.18814) : The research paper detailing the Mini-Gemini framework, published on arXiv, offering a comprehensive overview of the methodology and experimental results.

## Topics

![](topics/Model/Mini%20Gemini)