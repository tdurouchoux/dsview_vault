---
already_read: false
link: https://github.com/pytorch/torchtune
read_priority: 1
relevance: 0
source: null
tags:
- Large_Language_Model
- Development_tool
type: Content
upload_date: '2024-04-29'
---

https://github.com/pytorch/torchtune
## Summary

Torchtune is a PyTorch library designed for post-training large language models (LLMs). It offers hackable training recipes for various techniques such as Supervised Finetuning (SFT), Knowledge Distillation (KD), Reinforcement Learning (RL), and Quantization-Aware Training (QAT). The library supports popular LLMs like Llama, Gemma, Mistral, Phi, and Qwen, with configurations for different model sizes and training methods. It emphasizes memory efficiency and performance improvements, utilizing the latest PyTorch APIs. Torchtune provides YAML configs for easy setup of training, evaluation, quantization, or inference recipes. The library is compatible with various hardware and integrates with tools like Hugging Face Hub, EleutherAI's LM Eval Harness, and Weights & Biases. It is released under the BSD 3 license and encourages community contributions.
## Links

- [PyTorch TorchTune Recipes Overview](https://pytorch.org/torchtune/main/recipes/recipes_overview.html) : This link points to the recipes overview page of the TorchTune library, which provides various training recipes for LLMs.
- [PyTorch TorchTune Memory Optimizations](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html) : This link points to the memory optimizations tutorial page of the TorchTune library, which provides information on how to optimize memory usage during training.
- [PyTorch TorchTune End-to-End Workflow Tutorial](https://pytorch.org/torchtune/main/tutorials/e2e_flow.html) : This link points to the end-to-end workflow tutorial page of the TorchTune library, which provides a step-by-step guide on how to use the library for training and evaluating LLMs.
- [Hugging Face Phi-3 Model Collection](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) : This link points to the Phi-3 model collection page on the Hugging Face Hub, which provides access to the model weights and other resources for the Phi-3 model.
- [Hugging Face Gemma2 Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/gemma2) : This link points to the Gemma2 model documentation page on the Hugging Face Hub, which provides information on how to use the Gemma2 model for training and inference.

## Topics

![](topics/Library/torchtune)

![](topics/Concept/LoRA)

![](topics/Concept/QLoRA)

![](topics/Concept/Supervised%20Fine%20Tuning%20SFT)

![](topics/Concept/Knowledge%20Distillation)

![](topics/Concept/Reinforcement%20Learning%20from%20Human%20Feedback%20RLHF)

![](topics/Concept/Direct%20Preference%20Optimization%20DPO)

![](topics/Concept/Quantization%20Aware%20Training%20QAT)