---
already_read: true
link: https://github.com/deepseek-ai/DeepSeek-V3
read_priority: 1
relevance: 0
source: Alpha Signal
tags:
- Large_Language_Model
type: Content
upload_date: '2024-12-28'
---

https://github.com/deepseek-ai/DeepSeek-V3
## Summary

DeepSeek-V3 is a powerful Mixture-of-Experts (MoE) language model with 671B total parameters, activating 37B per token. It employs Multi-head Latent Attention (MLA) and DeepSeekMoE architectures for efficient inference and cost-effective training. Key innovations include an auxiliary-loss-free strategy for load balancing and a multi-token prediction training objective. Pre-trained on 14.8 trillion tokens, it undergoes Supervised Fine-Tuning and Reinforcement Learning stages. DeepSeek-V3 outperforms other open-source models and matches leading closed-source models, requiring only 2.788M H800 GPU hours for training. It supports various inference methods, including DeepSeek-Infer Demo, SGLang, LMDeploy, TensorRT-LLM, vLLM, and LightLLM, with compatibility for FP8 and BF16 precision. The model is licensed under MIT for code and a specific Model License for usage, supporting commercial applications.
## Links

- [DeepSeek-V3-Base Model on Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base) : This link points to the DeepSeek-V3-Base model on Hugging Face, providing access to the model weights and related resources.
- [DeepSeek Platform API](https://platform.deepseek.com/) : This link directs to the DeepSeek Platform, offering an OpenAI-Compatible API for interacting with the DeepSeek-V3 model.
- [DeepSeek Chat Website](https://chat.deepseek.com/) : This link leads to the official DeepSeek chat website, where users can interact with the DeepSeek-V3 model.
- [DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437) : This link provides access to the technical report of DeepSeek-V3, detailing the model's architecture, training, and evaluation results.

## Topics

![](topics/Model/DeepSeek%20V3)

![](topics/Concept/Mixture%20of%20Experts%20MoE)

![](topics/Concept/Multi%20head%20Latent%20Attention%20MLA)

![](topics/Concept/Multi%20Token%20Prediction%20MTP)

![](topics/Concept/Knowledge%20Distillation)

![](topics/Concept/Reinforcement%20Learning%20from%20Human%20Feedback%20RLHF)