---
already_read: true
link: https://github.com/deepseek-ai/DeepSeek-V3
read_priority: 1
relevance: 0
source: Alpha Signal
tags:
- Large_Language_Model
type: Content
upload_date: '2024-12-28'
---

https://github.com/deepseek-ai/DeepSeek-V3
## Summary

DeepSeek-V3 is a strong Mixture-of-Experts (MoE) language model with 671B total parameters, activating 37B for each token. It employs Multi-head Latent Attention (MLA) and DeepSeekMoE architectures for efficient inference and cost-effective training. Key innovations include an auxiliary-loss-free strategy for load balancing and a multi-token prediction training objective. Pre-trained on 14.8 trillion tokens, it undergoes Supervised Fine-Tuning and Reinforcement Learning stages. DeepSeek-V3 outperforms other open-source models and matches leading closed-source models, requiring only 2.788M H800 GPU hours for full training. It supports various inference methods, including DeepSeek-Infer Demo, SGLang, LMDeploy, TensorRT-LLM, vLLM, and LightLLM, with compatibility for FP8 and BF16 precision. The model is licensed under MIT for code and a specific Model License for usage, supporting commercial applications.
## Links

- [DeepSeek-V3 Model Downloads](https://huggingface.co/deepseek-ai/DeepSeek-V3) : This link provides access to the DeepSeek-V3 model downloads on Hugging Face, including the base and chat models with 671B total parameters.
- [DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437) : This link leads to the technical report of DeepSeek-V3, detailing the architecture, training, and evaluation results of the model.
- [DeepSeek Platform](https://platform.deepseek.com/) : This link directs to the DeepSeek Platform, where you can access the OpenAI-Compatible API for DeepSeek-V3 and other related services.
- [DeepSeek Chat Website](https://chat.deepseek.com/) : This link takes you to the official chat website of DeepSeek-V3, where you can interact with the model directly.

## Topics

![](topics/Model/DeepSeek%20V3)

![](topics/Concept/Multi%20head%20Latent%20Attention%20MLA)

![](topics/Concept/DeepSeekMoE)

![](topics/Concept/Multi%20Token%20Prediction%20MTP)