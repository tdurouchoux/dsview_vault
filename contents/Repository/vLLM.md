---
already_read: false
link: https://docs.vllm.ai/en/latest/
read_priority: 1
relevance: 0
source: null
tags:
- Large_Language_Model
type: Content
upload_date: '2024-05-05'
---

https://docs.vllm.ai/en/latest/
## Summary

vLLM is a library for efficient and scalable large language model (LLM) inference and serving. It is designed to optimize serving throughput, manage attention key and value memory with PagedAttention, and support continuous batching of incoming requests. Key features include fast model execution with CUDA/HIP graph, quantization support (GPTQ, AWQ, INT4, INT8, FP8), speculative decoding, and chunked prefill. vLLM integrates seamlessly with popular HuggingFace models and supports various decoding algorithms, tensor and pipeline parallelism, streaming outputs, and an OpenAI-compatible API server. It is compatible with a wide range of hardware, including NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs, GaudiÂ® accelerators and GPUs, IBM Power CPUs, TPU, and AWS Trainium and Inferentia Accelerators. The project is community-driven and includes contributions from both academia and industry.
## Links

- [vLLM Blog](https://blog.vllm.ai) : The official blog of vLLM, featuring articles and updates about the project.
- [vLLM Paper (SOSP 2023)](https://arxiv.org/abs/2306.00978) : The research paper detailing the architecture and performance of vLLM, published in SOSP 2023.
- [vLLM Roadmap](https://roadmap.vllm.ai) : The roadmap outlining the future development plans and goals for the vLLM project.

## Topics

![](topics/Library/vLLM)