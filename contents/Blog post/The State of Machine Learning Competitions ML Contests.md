---
already_read: false
link: https://mlcontests.com/state-of-machine-learning-competitions-2024/
read_priority: 4
relevance: 0
source: Data Elixir
tags:
- MlOps
- Large_Language_Model
- AI_agent
type: Content
upload_date: '2025-03-04'
---

https://mlcontests.com/state-of-machine-learning-competitions-2024/
## Summary

Over 400 ML competitions took place in 2024, with a total prize pool of over $22 million, hosted on more than 20 different platforms. There was a resurgence of 'grand challenge' style competitions with over $1 million in prize money each.

Python remained the dominant language among winners, with PyTorch and gradient-boosted tree models being the most common. Quantization proved key in winning solutions for LLM-related competitions, which tested skills like reasoning and information retrieval. AutoML packages showed value in narrow applications, but reports of Kaggle Grandmaster-level 'agents' were deemed premature. The ARC Prize competition was used as a benchmark for evaluating the reasoning capabilities of frontier LLMs.

The competition landscape included platforms like Kaggle, CodaLab, and Codabench, with Kaggle remaining the largest by registered users and total prize money. Grand challenges such as the AI Cyber Challenge and the Vesuvius Challenge attracted significant attention and funding. Corporates, nonprofits, and academic institutions also hosted numerous competitions focused on solving specific problems or hiring data scientists.

Winning solutions often utilized Python packages like NumPy, Pandas, and PyTorch, with new additions including Polars, TRL, and Accelerate. Deep learning libraries like PyTorch and TensorFlow were prevalent, with PyTorch being the most popular. Computer vision architectures such as U-Net, ConvNeXt, and EfficientNet were commonly used. NVIDIA GPUs were the most popular among winners, with the A100 being the most frequently used model.

Team demographics showed that over half of the winning teams were individuals, with repeat winners being relatively rare. Winning solutions in NLP and sequence data often involved decoder models like Llama, Mistral, and Gemma, with quantization and model size being crucial factors. Mathematics and reasoning competitions saw significant progress, with the AI Mathematical Olympiad and the ARC Prize being notable examples. Time series and tabular data competitions were dominated by gradient-boosted trees and deep learning methods.

AutoML competitions showed the value of automated machine learning techniques, with packages like LightAutoML and AutoGluon being used by winning teams. The concept of 'grandmaster-level agents' was explored, but current systems were found to be not yet capable of competing at that level. External and synthetic data, as well as the use of API-gated models, were also discussed as part of winning strategies.

Notable competitions included the AI Mathematical Olympiad, the ARC Prize, the AI Cyber Challenge, and the Vesuvius Challenge. The report looked ahead to trends like inference-time scaling and ongoing and future competitions. The methodology involved gathering data from competition platforms, organizers, and winners, with specific criteria for inclusion. The report concluded with acknowledgments and attribution information.
## Links

- [ARC Prize 2024 Technical Report](https://arxiv.org/abs/2412.04604) : Technical report detailing the ARC Prize 2024 competition, including winning solutions and approaches used by top teams.
- [Rasterio Documentation](https://rasterio.readthedocs.io/en/stable/?ref=mlcontests) : Documentation for Rasterio, a tool for dealing with geospatial raster data such as satellite imagery and terrain models in the TIF format.
- [Kaggle User Profile](https://www.kaggle.com/hydantess?ref=mlcontests) : Profile of a Kaggle user who has won multiple competitions, providing insights into their approaches and techniques.

## Topics

![](topics/Concept/Quantization)

![](topics/Model/Gradient%20Boosting%20Machines%20GBMs)

![](topics/Concept/AutoML)

![](topics/Library/Polars)