---
already_read: false
link: https://huggingface.co/blog/rlhf
read_priority: 1
relevance: 0
source: null
tags:
- Natural_Language_Processing
type: Content
upload_date: '2023-09-21'
---

https://huggingface.co/blog/rlhf
## Summary

Reinforcement Learning from Human Feedback (RLHF) is a method that uses human feedback to optimize language models, aligning them with complex human values. The process involves three main steps: pretraining a language model, gathering data to train a reward model, and fine-tuning the language model with reinforcement learning.

1. **Pretraining Language Models**: RLHF starts with a pretrained language model, which can be fine-tuned on additional text or conditions. The choice of the initial model is not clearly defined, and the design space of options in RLHF training is not thoroughly explored.

2. **Reward Model Training**: The goal is to create a model that takes in a sequence of text and returns a scalar reward representing human preference. The training dataset is generated by sampling prompts from a predefined dataset and using human annotators to rank the generated text outputs. The rankings are normalized into a scalar reward signal for training.

3. **Fine-tuning with RL**: The original language model is fine-tuned using a policy-gradient RL algorithm, Proximal Policy Optimization (PPO). The reward function combines the preference model and a constraint on policy shift, using a penalty based on the Kullback–Leibler (KL) divergence between the sequences of distributions over tokens. The final reward is used to update the model parameters.

Open-source tools for RLHF include TRL, TRLX, and RL4LMs, which offer various features for fine-tuning and evaluating language models with RL algorithms. The future of RLHF involves exploring new RL algorithms, improving the RL optimizer, and addressing the challenges of data collection and human annotation.

The article also provides a list of prevalent papers on RLHF, highlighting the growing interest and research in this field. The field is the convergence of multiple areas, including continual learning, bandit learning, and earlier work on using RL algorithms for text generation.
## Links

- [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) : The Kullback–Leibler divergence is a measure from information theory that quantifies the difference between two probability distributions. It is widely used in machine learning and statistics to measure how one probability distribution diverges from a second, expected probability distribution.
- [Implicit Language Q-Learning](https://sea-snell.github.io/ILQL_site/) : Implicit Language Q-Learning (ILQL) is a reinforcement learning algorithm designed for language models. It is particularly well-suited for optimizing language models with human feedback, making it relevant for RLHF applications.
- [TRL GitHub Repository](https://github.com/lvwerra/trl) : The TRL (Transformers Reinforcement Learning) repository provides tools and libraries for fine-tuning pretrained language models using reinforcement learning techniques, specifically Proximal Policy Optimization (PPO).
- [RL4LMs GitHub Repository](https://github.com/allenai/RL4LMs) : The RL4LMs (Reinforcement Learning for Language Models) repository offers building blocks for fine-tuning and evaluating language models with various reinforcement learning algorithms, including PPO, NLPO, A2C, and TRPO.

## Topics

![](topics/Concept/Pre%20training)

![](topics/Concept/Reward%20Modeling)

![](topics/Concept/Fine%20tuning%20with%20RL)

![](topics/Concept/Open%20source%20tools%20for%20RLHF)

![](topics/Concept/Proximal%20Policy%20Optimization%20PPO)

![](topics/Concept/Reinforcement%20Learning%20from%20Human%20Feedback%20RLHF)