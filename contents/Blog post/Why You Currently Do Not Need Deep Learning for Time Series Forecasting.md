---
already_read: false
link: https://readmedium.com/en/why-you-currently-do-not-need-deep-learning-for-time-series-forecasting-0de57f2bc0ed
read_priority: 1
relevance: 0
source: null
tags:
- Time_Series
- Supervised_Learning
type: Content
upload_date: '2024-07-03'
---

https://readmedium.com/en/why-you-currently-do-not-need-deep-learning-for-time-series-forecasting-0de57f2bc0ed
## Summary

The article argues that deep learning is not currently necessary for time series forecasting, highlighting several key points based on the Makridakis M5 competitions and the 2023 Kaggle AI report:

1. **ML Models Dominate**: Gradient Boosting Machines (GBMs) like LightGBM, XGBoost, and CatBoost outperform deep learning models in both accuracy and training time. These models are favored for their ease of use, speed, and effectiveness.

2. **Value of Statistical Methods**: Despite the rise of ML, simple statistical baseline models remain valuable. They are quick to develop and often provide strong initial results, helping to validate the need for more complex models.

3. **Ensemble Methods**: Combining different models can improve forecast accuracy, but simplicity is key to maintain practicality in real-world applications.

4. **Gap Between Theory and Practice**: Deep learning models, while popular in scientific literature, are not widely adopted in practical time series forecasting due to their complexity and performance issues.

5. **Feature Engineering**: The quality of features is more critical than the choice of model. Effective feature engineering, including the use of exogenous variables, significantly boosts model performance.

6. **Iteration and Cross-Validation**: Fast iteration and robust cross-validation strategies are essential for identifying the best features and models. Each problem requires a unique approach tailored to its specific characteristics.

The article concludes that deep learning is not yet practical for real-world time series forecasting, emphasizing the importance of ML models, feature engineering, and domain-specific solutions.
## Links

- [N-BEATS — The First Interpretable Deep Learning Model That Worked for Time Series Forecasting](https://towardsdatascience.com/n-beats-the-first-interpretable-deep-learning-model-that-worked-for-time-series-forecasting-06920daadac2) : An easy-to-understand deep dive into how N-BEATS works and how you can use it.
- [N-HiTS — Making Deep Learning for Time Series Forecasting More Efficient](https://towardsdatascience.com/n-hits-making-deep-learning-for-time-series-forecasting-more-efficient-d00956fc3e93) : A deep dive into how N-HiTS works and how you can use it
- [Why You Should Always Start With a Baseline Model](https://pub.towardsai.net/why-you-should-always-start-with-a-baseline-model-95d78c70941c) : A baseline model takes 10 % of the time to develop but gets us 90 % of the way to achieve reasonable results.

## Topics

![[topics/Concept/Feature Engineering]]

![[topics/Concept/Ensemble Methods]]

![[topics/Model/XGBoost]]

![[topics/Concept/Exogenous Variables]]

![[topics/Model/LightGBM]]

![[topics/Model/CatBoost]]

![[topics/Concept/Cross Validation]]

![[topics/Model/Gradient Boosting Machines GBMs]]