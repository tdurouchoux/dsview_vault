---
already_read: false
link: https://magazine.sebastianraschka.com/p/understanding-reasoning-llms
read_priority: 3
relevance: 0
source: Data Elixir
tags:
- Large_Language_Model
- AI_agent
type: Content
upload_date: '2025-02-18'
---

https://magazine.sebastianraschka.com/p/understanding-reasoning-llms
## Summary

The article discusses the evolution and specialization of large language models (LLMs) in 2024, with a focus on reasoning models. It defines reasoning models as those capable of complex, multi-step tasks like puzzles, advanced math, and coding challenges. The author outlines four main approaches to building and improving reasoning models:

1. **Inference-time scaling**: Enhancing model performance during inference by using techniques like chain-of-thought prompting, voting, and search strategies. This approach increases computational resources and encourages the model to generate intermediate reasoning steps.

2. **Pure reinforcement learning (RL)**: Demonstrated by DeepSeek-R1-Zero, this approach shows that reasoning can emerge from pure RL without supervised fine-tuning (SFT). The model uses accuracy and format rewards to develop basic reasoning skills.

3. **Supervised fine-tuning (SFT) and reinforcement learning (RL)**: DeepSeek-R1, the flagship model, combines SFT and RL to improve reasoning performance. This approach includes additional SFT stages and RL training with accuracy, format, and consistency rewards.

4. **Pure supervised fine-tuning (SFT) and distillation**: DeepSeek-R1-Distill involves fine-tuning smaller models like Llama and Qwen on SFT data generated by larger models. This approach aims to create smaller, more efficient models with strong reasoning capabilities.

The article also discusses the cost and efficiency of developing reasoning models, highlighting the potential of distillation and low-budget approaches like Sky-T1 and TinyZero. It concludes with insights on prompting reasoning models and the future of reasoning model development.
## Links

- [DeepSeek R1 Technical Report](https://arxiv.org/abs/2501.12948) : The technical report detailing the development and performance of the DeepSeek R1 reasoning models.
- [Scaling LLM Test-Time Compute](https://arxiv.org/abs/2408.03314) : A paper discussing various strategies for scaling LLM test-time compute to improve model performance.
- [O1 Replication Journey](https://arxiv.org/abs/2410.18982) : A strategic progress report introducing journey learning as an alternative to traditional shortcut learning for improving distillation (pure SFT) process.
- [Sky-T1 Project](https://novasky-ai.github.io/posts/sky-t1/) : An article about the Sky-T1 project, which trained an open-weight 32B model using only 17K SFT samples at a cost of $450.
- [TinyZero Repository](https://github.com/Jiayi-Pan/TinyZero/) : A repository for the TinyZero project, which replicates the DeepSeek-R1-Zero approach with a 3B parameter model, demonstrating emergent self-verification abilities.

## Topics

![](topics/Concept/Reasoning%20Models)

![](topics/Concept/Inference%20time%20scaling)

![](topics/Concept/Reinforcement%20Learning%20RL)

![](topics/Concept/Knowledge%20Distillation)

![](topics/Concept/Chain%20of%20Thought%20Prompting)

![](topics/Concept/Journey%20Learning)

![](topics/Concept/Supervised%20Fine%20Tuning%20SFT)