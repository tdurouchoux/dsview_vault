---
already_read: false
link: https://magazine.sebastianraschka.com/p/understanding-reasoning-llms
read_priority: 3
relevance: 0
source: Data Elixir
tags:
- Large_Language_Model
- AI_agent
type: Content
upload_date: '2025-02-18'
---

https://magazine.sebastianraschka.com/p/understanding-reasoning-llms
## Summary

The article discusses the development and refinement of reasoning models, which are specialized large language models (LLMs) designed for complex, multi-step tasks. The author outlines four main approaches to building and improving these models:

1. **Inference-time scaling**: This involves increasing computational resources during inference to enhance output quality. Techniques include chain-of-thought (CoT) prompting, voting, and search strategies like beam search.

2. **Pure reinforcement learning (RL)**: The DeepSeek-R1-Zero model demonstrates that reasoning can emerge from pure RL without initial supervised fine-tuning (SFT). This approach uses accuracy and format rewards to guide the model.

3. **Supervised fine-tuning (SFT) and reinforcement learning (RL)**: The DeepSeek-R1 model improves upon R1-Zero by incorporating additional SFT and RL stages, including accuracy, format, and consistency rewards. This approach leads to significant performance improvements.

4. **Pure SFT and distillation**: Smaller models like DeepSeek-R1-Distill are trained using SFT data generated by larger models. This method is cost-effective and suitable for smaller-scale applications.

The article also compares DeepSeek-R1 with OpenAI's o1, noting that while both are strong, DeepSeek-R1 is more efficient at inference time. The author discusses the cost of training such models and highlights cost-effective alternatives like distillation and pure RL approaches for budget-conscious developers. Additionally, the article mentions the potential of journey learning, which includes incorrect solution paths in SFT data to improve model reliability.
## Links

- [DeepSeek R1 Technical Report](https://arxiv.org/abs/2501.12948) : Technical report detailing the development and performance of the DeepSeek R1 reasoning models.
- [Scaling LLM Test-Time Compute](https://arxiv.org/abs/2408.03314) : Paper discussing strategies for optimizing LLM performance through test-time compute scaling.
- [Sky-T1 Training Article](https://novasky-ai.github.io/posts/sky-t1/) : Article describing the training of an open-weight 32B model using a limited budget, achieving performance comparable to o1.
- [TinyZero Repository](https://github.com/Jiayi-Pan/TinyZero/) : Repository for TinyZero, a 3B parameter model demonstrating emergent self-verification abilities through pure RL.
- [O1 Replication Journey](https://arxiv.org/abs/2410.18982) : Paper introducing journey learning as an alternative to traditional shortcut learning for improving distillation processes in LLMs.

## Topics

![](topics/Concept/Reasoning%20Models)

![](topics/Concept/Inference%20time%20Scaling)

![](topics/Concept/Reinforcement%20Learning)

![](topics/Concept/Supervised%20Fine%20Tuning)

![](topics/Concept/Knowledge%20Distillation)

![](topics/Concept/Chain%20of%20Thought%20CoT%20Reasoning)

![](topics/Concept/Journey%20Learning)