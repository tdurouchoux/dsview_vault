---
already_read: false
link: https://huggingface.co/blog/smolervlm
read_priority: 4
relevance: 0
source: null
tags:
- Large_Language_Model
- Computer_Vision
type: Content
upload_date: '2025-03-03'
---

https://huggingface.co/blog/smolervlm
## Summary

Hugging Face introduces two new Vision Language Models (VLMs), SmolVLM-256M and SmolVLM-500M, with 256 million and 500 million parameters respectively. These models are designed to be smaller, more efficient, and capable of strong multimodal performance. Key features include a smaller vision encoder (SigLIP base patch-16/512), larger image resolution for sharper understanding, and optimized tokenization for better real-world benchmarks. The models are available in four checkpoints and can be used with transformers, MLX, and ONNX. They are suitable for tasks like captioning, document Q&A, and basic visual reasoning. The release also includes ColSmolVLM for multimodal retrieval and partnerships for document understanding. The models are designed to be used out-of-the-box and can be fine-tuned for specific tasks. The team behind the models has also provided resources for fine-tuning and multimodal RAG with ColSmolVLM.
## Links

- [SmolVLM-500M-Instruct-WebGPU](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-500M-Instruct-WebGPU) : WebGPU demo for the SmolVLM-500M-Instruct model.
- [SmolVLM-256M-Instruct-WebGPU](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-256M-Instruct-WebGPU) : WebGPU demo for the SmolVLM-256M-Instruct model.
- [SmolVLM-256M and 500M Models Collection](https://huggingface.co/collections/HuggingFaceTB/smolvlm-256m-and-500m-6791f4f5bb0ab8acc960fb0) : Collection of SmolVLM-256M and 500M models, including base and instruction fine-tuned versions.

## Topics

![[topics/Concept/Tokenization)]]

![[topics/Model/SmolVLM)]]

![[topics/Model/SmolVLM 256M)]]

![[topics/Concept/Data Mixture)]]

![[topics/Concept/Multimodal Retrieval)]]

![[topics/Concept/Vision Encoder)]]