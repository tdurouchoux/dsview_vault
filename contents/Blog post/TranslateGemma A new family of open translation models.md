---
already_read: true
link: https://blog.google/innovation-and-ai/technology/developers-tools/translategemma/
read_priority: 0
relevance: 4
source: null
tags:
- Natural_Language_Processing
type: Content
upload_date: '2026-02-01'
---

https://blog.google/innovation-and-ai/technology/developers-tools/translategemma/
## Summary

TranslateGemma is a new suite of open translation models built on Gemma 3, available in 4B, 12B, and 27B parameter sizes. It supports communication across 55 languages with high efficiency and quality. The 12B model outperforms the Gemma 3 27B baseline using fewer parameters, making it ideal for high-fidelity translation with lower latency. The models are trained using a two-stage process involving supervised fine-tuning and reinforcement learning, ensuring broad language coverage and high fidelity. TranslateGemma retains multimodal capabilities, effectively translating text within images. It is designed for diverse deployment environments, from mobile devices to cloud-based systems. The models can be downloaded from Kaggle or Hugging Face and deployed in Vertex AI.
## Links

- [TranslateGemma on Hugging Face](https://huggingface.co/collections/google/translategemma) : Download TranslateGemma on Hugging Face
- [TranslateGemma on Kaggle](https://www.kaggle.com/models/google/translategemma/) : Download TranslateGemma on Kaggle
- [TranslateGemma Technical Report](https://arxiv.org/pdf/2601.09012) : Read the technical report of TranslateGemma

## Topics

![[topics/Model/TranslateGemma]]

![[topics/Model/Gemma]]

![[topics/Concept/Supervised Fine Tuning]]

![[topics/Concept/Reinforcement Learning]]

![[topics/Concept/MetricX]]

![[topics/Concept/WMT24 Benchmark]]

![[topics/Concept/Multimodality]]